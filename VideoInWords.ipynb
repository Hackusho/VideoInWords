{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","mount_file_id":"1e6ieKHb7b2Y6PzImZYicPXEakQmFWwXR","authorship_tag":"ABX9TyP+wUqlrw3koVmie+PHDgAM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1d5a196b3cc0440e983e6cb2bea6e716":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c2624569df04efaa50fcab557c42e60","IPY_MODEL_5d23d9efe1084eb48fcc2e954f15da0e","IPY_MODEL_70f8acef78d54c94bdd4b2623032d1e6"],"layout":"IPY_MODEL_a7254607d2144eaf829983b49184acfe"}},"5c2624569df04efaa50fcab557c42e60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1e6f8d5b9874d7aa530f51dbf6c4fc7","placeholder":"​","style":"IPY_MODEL_bda4894e023c4397a354e2f1442c04ac","value":"Map: 100%"}},"5d23d9efe1084eb48fcc2e954f15da0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5df002ed6d2c4ace8ad6cba724319e9f","max":400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97d7637e644843a8a447a2a3d0616923","value":400}},"70f8acef78d54c94bdd4b2623032d1e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb07b58b065d4c5cbc899d22b2034d13","placeholder":"​","style":"IPY_MODEL_88006debf8ad4439944040bc4c23ecaa","value":" 400/400 [11:41&lt;00:00,  1.35 examples/s]"}},"a7254607d2144eaf829983b49184acfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1e6f8d5b9874d7aa530f51dbf6c4fc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bda4894e023c4397a354e2f1442c04ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5df002ed6d2c4ace8ad6cba724319e9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97d7637e644843a8a447a2a3d0616923":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb07b58b065d4c5cbc899d22b2034d13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88006debf8ad4439944040bc4c23ecaa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Fine-Tuning BLIP on IIW and Comparing to Baseline Model\n","This notebook fine-tunes BLIP on ImageInWords (IIW) dataset, to generate hyper-detailed descriptions.\n","\n","##Install the Datasets Library\n"],"metadata":{"id":"vBX37_8nuo4_"}},{"cell_type":"code","source":["!pip install datasets accelerate torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"HkODfRi4uofq","executionInfo":{"status":"ok","timestamp":1745807666976,"user_tz":240,"elapsed":73092,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"ccbac9f7-a5d5-4038-d6af-422f41aa16d7"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["nvidia"]},"id":"8c4b16c43a4648099137ac08065877e1"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install transformers==4.40.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":738},"id":"Cn9Bf7uquM_3","executionInfo":{"status":"ok","timestamp":1745807676866,"user_tz":240,"elapsed":9883,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"9752beea-e91e-4948-86b8-969327e81840"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.40.0\n","  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2.32.3)\n","Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.0)\n","  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2024.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2025.1.31)\n","Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.1\n","    Uninstalling tokenizers-0.21.1:\n","      Successfully uninstalled tokenizers-0.21.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.51.3\n","    Uninstalling transformers-4.51.3:\n","      Successfully uninstalled transformers-4.51.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["transformers"]},"id":"5133024fa423463992ce2c005581b64d"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install --upgrade transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":703},"id":"U1l-v82B8q7f","executionInfo":{"status":"ok","timestamp":1745807687032,"user_tz":240,"elapsed":10163,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"7d53247f-1ad1-4254-c8bc-a50616395efa"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.40.0)\n","Collecting transformers\n","  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Collecting tokenizers<0.22,>=0.21 (from transformers)\n","  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m138.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.40.0\n","    Uninstalling transformers-4.40.0:\n","      Successfully uninstalled transformers-4.40.0\n","Successfully installed tokenizers-0.21.1 transformers-4.51.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["transformers"]},"id":"c3c20cdb2e9246ceabe340740aeb735b"}},"metadata":{}}]},{"cell_type":"markdown","source":["#Load Image In Words(IIW) Dataset"],"metadata":{"id":"eOWq5z0Ivsly"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"91JJqP2IsPb3","executionInfo":{"status":"ok","timestamp":1745807738446,"user_tz":240,"elapsed":13,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}}},"outputs":[],"source":["import json\n","\n","dataset_path = '/content/drive/MyDrive/NLP Final Project/data/data.jsonl'\n","\n","data = []\n","with open(dataset_path, 'r') as f:\n","    for line in f:\n","        data.append(json.loads(line))\n"]},{"cell_type":"markdown","source":["Sample Data from IIW (All String entries)"],"metadata":{"id":"mWlZigOtzDVD"}},{"cell_type":"code","source":["print(f\"Loaded {len(data)} entries!\")\n","print(data[0])  # see the first sample\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qm73s7-fv2cH","executionInfo":{"status":"ok","timestamp":1745807740255,"user_tz":240,"elapsed":8,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"8e06a155-f6de-4f35-cbfb-e0251040a88a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 400 entries!\n","{'IIW': \"A close-up outdoor shot shows an Echinops Bannaticus Blue Glow Globe flower in front of other flowers of that ilk in front of a blue sky and an out-of-focus background. The flower's spiky extensions are light purple-blue in color with curled tips of dark brown. The stem is pale tan and appears to be fuzzy. At the bottom-left is a clear focused dark green leaf directed toward the viewer with side leaves going out, one to each side. But the bloom high above those leaves is out-of-focus and dark.\", 'IIW-P5B': \"A detailed close-up captures a bumble bee perched atop a thistle flower, set against a softly blurred backdrop of a clear blue sky. The bee, adorned in a striped coat of yellow and white, is perched on the flower's light brown stem, which extends from the bottom left to the center of the frame. The thistle's purple filaments, in sharp focus, contrast with the blurred background, creating a bokeh effect that draws the viewer's attention to the foreground subject. In the bottom right corner of the frame, another thistle flower peeks into view, its purple filaments also in sharp focus.\", 'iiw-human-sxs-gpt4v': {'metrics/Comprehensiveness': 'GPT-4V is marginally better', 'metrics/First few line(s) as tldr': 'IIW-Human is marginally better', 'metrics/Hallucination': 'IIW-Human is marginally better', 'metrics/Human Like': 'GPT-4V is marginally better', 'metrics/Specificity': 'GPT-4V is marginally better'}, 'iiw-human-sxs-iiw-p5b': {'metrics/Comprehensiveness': 'IIW-P5B is marginally better', 'metrics/First few line(s) as tldr': 'Neutral', 'metrics/Hallucination': 'IIW-Human is marginally better', 'metrics/Human Like': 'Neutral', 'metrics/Specificity': 'IIW-Human is marginally better'}, 'image/key': 'aar_test_04600', 'objects': [{'description': 'A group of small echinops bannaticus purple flower plants has a bee sitting on the flower in the middle, slightly to the right. There are green leaves below the flowers.', 'label': 'Echinops bannaticus flowers', 'normalized_coords': ['1', '0', '998', '999']}, {'description': 'A black and yellow stripe has a white bottom on it.', 'label': 'Bumble bee', 'normalized_coords': ['537', '490', '747', '814']}, {'description': 'A blurry plain blue sky is behind the group of flowers.', 'label': 'Sky', 'normalized_coords': ['2', '0', '546', '999']}]}\n"]}]},{"cell_type":"markdown","source":["Convert to Dataset object"],"metadata":{"id":"QXwY-mz1ohS_"}},{"cell_type":"code","source":["from datasets import Dataset\n","\n","# Convert list of dicts to Hugging Face Dataset\n","dataset = Dataset.from_list(data)"],"metadata":{"id":"HJ-vLDAH3jfu","executionInfo":{"status":"ok","timestamp":1745807743217,"user_tz":240,"elapsed":913,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Add corresponding images"],"metadata":{"id":"8briqpFZoth0"}},{"cell_type":"code","source":["from PIL import Image\n","import os\n","\n","# Load images based on 'image/key'\n","image_folder = '/content/drive/MyDrive/NLP Final Project/data/images_aar'\n","\n","def load_image(example):\n","    image_path = os.path.join(image_folder, example[\"image/key\"] + '.jpg')\n","    if os.path.exists(image_path):\n","        example[\"image\"] = Image.open(image_path).convert(\"RGB\")\n","    else:\n","        example[\"image\"] = None  # optional: you could filter these out if missing\n","    example[\"caption\"] = example[\"IIW\"]  # human hyper-detailed description\n","    return example"],"metadata":{"id":"Y-0evaH8ovoi","executionInfo":{"status":"ok","timestamp":1745807744517,"user_tz":240,"elapsed":4,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Match images to captions"],"metadata":{"id":"yV2w-YOhRwau"}},{"cell_type":"code","source":["dataset = dataset.map(load_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["1d5a196b3cc0440e983e6cb2bea6e716","5c2624569df04efaa50fcab557c42e60","5d23d9efe1084eb48fcc2e954f15da0e","70f8acef78d54c94bdd4b2623032d1e6","a7254607d2144eaf829983b49184acfe","d1e6f8d5b9874d7aa530f51dbf6c4fc7","bda4894e023c4397a354e2f1442c04ac","5df002ed6d2c4ace8ad6cba724319e9f","97d7637e644843a8a447a2a3d0616923","bb07b58b065d4c5cbc899d22b2034d13","88006debf8ad4439944040bc4c23ecaa"]},"id":"vvP2jByeogTB","executionInfo":{"status":"ok","timestamp":1745808448573,"user_tz":240,"elapsed":701775,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"de911677-7313-4e34-8e80-d626cb9abc86"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/400 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d5a196b3cc0440e983e6cb2bea6e716"}},"metadata":{}}]},{"cell_type":"markdown","source":["# Preprocess Data"],"metadata":{"id":"94F8N7oJlmMg"}},{"cell_type":"code","source":["dataset = dataset.remove_columns([col for col in dataset.column_names if col not in [\"image\", \"caption\"]])\n","train_test = dataset.train_test_split(test_size=0.1)\n","train_dataset = train_test[\"train\"]\n","val_dataset = train_test[\"test\"]\n","\n","print(\"Training samples:\", len(train_dataset))\n","print(\"Validation samples:\", len(val_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkOCRzIqkYgi","executionInfo":{"status":"ok","timestamp":1745808448587,"user_tz":240,"elapsed":10,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"b79ad7e6-74ec-4f53-af71-f95cb250987f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Training samples: 360\n","Validation samples: 40\n"]}]},{"cell_type":"markdown","source":["Load BLIP Processor and Model"],"metadata":{"id":"zqGxcgHSzH78"}},{"cell_type":"code","source":["from transformers import BlipProcessor, BlipForConditionalGeneration\n","import torch\n","\n","# Load BLIP\n","model_id = \"Salesforce/blip-image-captioning-base\"\n","\n","processor = BlipProcessor.from_pretrained(model_id)\n","model = BlipForConditionalGeneration.from_pretrained(model_id)\n","\n","# Move model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HgvJoB62kBbY","executionInfo":{"status":"ok","timestamp":1745808458707,"user_tz":240,"elapsed":10118,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"ece50e05-92af-416e-b05f-b0c0017948b0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Function to Collate data"],"metadata":{"id":"dlKRp2RcrZDG"}},{"cell_type":"code","source":["def collate_fn(batch):\n","    images = [example[\"image\"] for example in batch]\n","    captions = [example[\"caption\"] for example in batch]\n","    inputs = processor(\n","        images=images,\n","        text=captions,\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=512,\n","        return_tensors=\"pt\"\n","    )\n","    inputs['labels'] = inputs.input_ids.clone()\n","    return inputs\n"],"metadata":{"id":"9BTi9TEnrYkc","executionInfo":{"status":"ok","timestamp":1745808458716,"user_tz":240,"elapsed":5,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KH0htZvBC_2","executionInfo":{"status":"ok","timestamp":1745808458737,"user_tz":240,"elapsed":17,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"3893a007-caae-4e0a-b6e3-a4a93b47a74a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["4.51.3\n"]}]},{"cell_type":"markdown","source":["Training arguements"],"metadata":{"id":"ojFE7kJCLeBS"}},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./blip_finetuned_iiw\",\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    logging_steps=10,\n","    num_train_epochs=5,\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n","    save_total_limit=1,\n","    fp16=True,\n","    report_to=\"none\",\n","    remove_unused_columns=False\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=collate_fn\n",")\n","\n"],"metadata":{"id":"E-zwoWIGtkrm","executionInfo":{"status":"ok","timestamp":1745808459661,"user_tz":240,"elapsed":922,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Train"],"metadata":{"id":"tjt1d_PExECG"}},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":800},"id":"5_Qn1eN9xDYh","executionInfo":{"status":"ok","timestamp":1745808863747,"user_tz":240,"elapsed":404084,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"6b886f73-5753-4ffb-dbec-edff3c7da0ac"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [225/225 06:40, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>9.663600</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>7.127100</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>6.209000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>5.505100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>4.825600</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>4.185700</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>3.656900</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>3.297100</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>2.727400</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>2.367100</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>2.105200</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>2.111100</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>1.831500</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>1.713400</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.638700</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>1.746200</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>1.584900</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>1.549200</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>1.475500</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.380600</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>1.449600</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>1.473900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=225, training_loss=3.126864585876465, metrics={'train_runtime': 403.9906, 'train_samples_per_second': 4.456, 'train_steps_per_second': 0.557, 'total_flos': 1.0681625740050432e+18, 'train_loss': 3.126864585876465, 'epoch': 5.0})"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Save Model"],"metadata":{"id":"F4WnZToELinz"}},{"cell_type":"code","source":["model.save_pretrained(\"/content/drive/MyDrive/NLP Final Project/models/blip_finetuned_iiw_final\")\n","processor.save_pretrained(\"/content/drive/MyDrive/NLP Final Project/models/blip_finetuned_iiw_final\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2sLIdir7ITd3","executionInfo":{"status":"ok","timestamp":1745808872375,"user_tz":240,"elapsed":8611,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"08bd1e77-11dd-4028-eabe-be535a8630ee"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["# Using The VIM Pipeline\n","\n","Install dependencies for video processing"],"metadata":{"id":"GP0YeHMpLrOO"}},{"cell_type":"code","source":["!pip install opencv-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kDABv73wLp9y","executionInfo":{"status":"ok","timestamp":1745808874599,"user_tz":240,"elapsed":2224,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"0c6608c3-eb98-4729-9ccf-95d525afb33a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"]}]},{"cell_type":"markdown","source":["Select Model (Vanilla BLIP, Fine-tuned BLIP, or Llama 4 SCOUT)"],"metadata":{"id":"0uVt9uE7MoMx"}},{"cell_type":"code","source":["import cv2\n","import os\n","from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline, AutoProcessor, AutoModelForVision2Seq\n","from PIL import Image\n","import torch\n","import difflib\n","\n","def load_model(model_choice):\n","    if model_choice == \"blip-vanilla\":\n","        path = \"Salesforce/blip-image-captioning-base\"\n","        processor = BlipProcessor.from_pretrained(path)\n","        model = BlipForConditionalGeneration.from_pretrained(path)\n","    elif model_choice == \"blip-finetuned\":\n","        path = \"/content/drive/MyDrive/NLP Final Project/models/blip_finetuned_iiw_final\"\n","        processor = BlipProcessor.from_pretrained(path)\n","        model = BlipForConditionalGeneration.from_pretrained(path)\n","    #elif model_choice == \"llama-4\":\n","        #path = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"  # Future Work - Comparison to SOTA (RELEASED 4/16/25)\n","        #processor = AutoProcessor.from_pretrained(path)\n","        #model = AutoModelForVision2Seq.from_pretrained(path)\n","    else:\n","        raise ValueError(\"Invalid model_choice: choose 'blip-finetuned' or 'blip-vanilla'\")\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    return processor, model, device"],"metadata":{"id":"opPbO6SXMcol","executionInfo":{"status":"ok","timestamp":1745808874712,"user_tz":240,"elapsed":49,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Helper Functions"],"metadata":{"id":"8OCzxOObM_Dw"}},{"cell_type":"code","source":["# Function to Extract Frames from Video\n","def extract_frames(video_path, output_folder, fps=1):\n","    print(\"VIW Activated! \")\n","    print(f\"Analyzing...\")\n","\n","    os.makedirs(output_folder, exist_ok=True)\n","    video = cv2.VideoCapture(video_path)\n","    frame_rate = video.get(cv2.CAP_PROP_FPS)\n","    frame_interval = int(frame_rate // fps)\n","\n","    print(\"Extracting Frames...\")\n","\n","    count = 0\n","    saved = 0\n","\n","    while True:\n","        ret, frame = video.read()\n","        if not ret:\n","            break\n","        if count % frame_interval == 0:\n","            frame_filename = os.path.join(output_folder, f\"frame_{saved:04d}.jpg\")\n","            cv2.imwrite(frame_filename, frame)\n","            saved += 1\n","        count += 1\n","\n","    video.release()\n","    print(f\"Extracted {saved} frames to {output_folder}\")\n","\n","# Function to Clean the Captions\n","def clean_caption(raw_caption):\n","    # Keep everything BEFORE the first </s>\n","    caption = raw_caption.split(\"</s>\")[0]\n","    caption = caption.replace(\"<s>\", \"\").strip()\n","\n","    # Make sure it ends cleanly\n","    if not caption.endswith('.'):\n","        caption += '.'\n","    caption = caption[0].upper() + caption[1:]\n","\n","    return caption\n","\n","\n","\n","# Function to Caption Each Frame\n","def caption_frames(frame_folder, processor, model, device):\n","    print(\"Captioning Frames...\")\n","    captions = []\n","    frames = sorted(os.listdir(frame_folder))\n","\n","    for idx, frame_name in enumerate(frames):\n","        img_path = os.path.join(frame_folder, frame_name)\n","        raw_image = Image.open(img_path).convert('RGB')\n","\n","        inputs = processor(images=raw_image, return_tensors=\"pt\").to(device)\n","        out = model.generate(**inputs, max_length=35, num_beams=3)\n","\n","        raw_caption = processor.decode(out[0], skip_special_tokens=False)  # DON'T skip tokens yet\n","        cleaned_caption = clean_caption(raw_caption)  #  run the smart cleaner\n","\n","        captions.append(cleaned_caption)\n","        print(f\" Frame {idx+1}: {cleaned_caption}\")\n","\n","    return captions\n","\n","\n","\n","# Funtion to merge frame captions that are at least a percentage different\n","def merge_captions(frame_captions_unique, similarity_threshold=0.05): # <<< Final Filtering Threshold (Story Smoothing)\n","    print(\"\\nMerging and Cleaning Captions into a Final Story...\")\n","\n","    unique_texts = []\n","    last_caption = \"\"\n","\n","    for frame_name, caption in frame_captions_unique:\n","        # Basic cleaning (First Cap and Period at the end)\n","        caption = caption.strip()\n","        if not caption.endswith('.'):\n","            caption += '.'\n","        caption = caption[0].upper() + caption[1:]\n","\n","        # Compare similarity to last accepted caption\n","        similarity = difflib.SequenceMatcher(None, caption.lower(), last_caption.lower()).ratio()\n","\n","        if similarity < similarity_threshold:\n","            unique_texts.append(caption)\n","            last_caption = caption\n","        else:\n","            print(f\"Skipped Similar Caption: {caption}\")\n","\n","    print('Unique Captions Used: ' + str(len(unique_texts)))\n","    print(unique_texts)\n","    combined_text = \" \".join(unique_texts)\n","\n","    print(\"\\nFinal Smoothed Video Caption:\")\n","    print(combined_text)\n","\n","    return combined_text\n","\n","# Helper funtion to Run the VIW pipeline\n","def run_viw_pipeline(video_path, model_choice, fps=.5):\n","    frames_folder = \"/content/frames_output/\"\n","\n","    # Step 1: Load model\n","    processor, model, device = load_model(model_choice)\n","\n","    # Step 2: Extract frames\n","    extract_frames(video_path, frames_folder, fps=fps)\n","\n","    # Step 3: Caption frames\n","    frame_captions_full = []   # All generated captions\n","    frames = sorted(os.listdir(frames_folder))\n","\n","    print(\"\\nGenerating Captions for Each Frame:\")\n","\n","    for idx, frame_name in enumerate(frames):\n","        img_path = os.path.join(frames_folder, frame_name)\n","        raw_image = Image.open(img_path).convert('RGB')\n","\n","        inputs = processor(images=raw_image, return_tensors=\"pt\").to(device)\n","        out = model.generate(**inputs, max_length=50)\n","        caption = processor.decode(out[0], skip_special_tokens=True)\n","\n","        frame_captions_full.append((frame_name, caption))\n","        print(f\"Frame {idx+1}: {caption}\")\n","\n","    # Step 4: Create Unique Captions based on similarity\n","    print(\"\\nFiltering Unique Captions Based on Similarity...\")\n","    frame_captions_unique = []\n","    last_caption = \"\"\n","\n","    for frame_name, caption in frame_captions_full:\n","        caption = caption.strip()\n","        similarity = difflib.SequenceMatcher(None, caption.lower(), last_caption.lower()).ratio()\n","\n","        if similarity < 0.25:  # <<< Apply similarity threshold here! ( Early filtering)\n","            frame_captions_unique.append((frame_name, caption))\n","            last_caption = caption\n","        else:\n","            print(f\" Skipped Similar Caption: {caption}\")\n","\n","    # Step 5: Show Unique Captions\n","    print(\"\\nUnique Captions After Filtering:\")\n","    for idx, (frame_name, caption) in enumerate(frame_captions_unique):\n","        print(f\"Unique {idx+1}: {caption}\")\n","\n","    # Step 6: Merge into Final Story\n","    combined_text = merge_captions(frame_captions_unique)\n","\n","    return frame_captions_full, frame_captions_unique, combined_text\n"],"metadata":{"id":"TulKA_ENNkzP","executionInfo":{"status":"ok","timestamp":1745808874731,"user_tz":240,"elapsed":17,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Run VideoInWords (VIW)"],"metadata":{"id":"xUtNn1FXfOHj"}},{"cell_type":"code","source":["# Define available models\n","model_choices = {\n","    \"Vanilla BLIP\": \"blip-vanilla\",\n","    \"Fine-Tuned BLIP\": \"blip-finetuned\"\n","    #\"LLaMA 4\": \"llama-4\"\n","}\n","\n","# Define available videos\n","video_choices = {\n","    \"A man and his dog walk through the snow in front of a cabin\": \"/content/drive/MyDrive/NLP Final Project/vids/002.mp4\",\n","    \"Close-up View of Chef Preparing Food\": \"/content/drive/MyDrive/NLP Final Project/vids/003.mp4\",\n","    \"Child Playing on a Tranquil Beach\": \"/content/drive/MyDrive/NLP Final Project/vids/004.mp4\",\n","    \"Busy Nightlife Street in Rain with Umbrellas\": \"/content/drive/MyDrive/NLP Final Project/vids/005.mp4\",\n","    \"Tourists at Teotihuacan's Pyramid of the Sun\": \"/content/drive/MyDrive/NLP Final Project/vids/006.mp4\"\n","\n","}\n","\n","# 1. Select Model\n","print(\"\\nAvailable Models:\")\n","for idx, model_name in enumerate(model_choices.keys()):\n","    print(f\"{idx+1}. {model_name}\")\n","\n","model_idx = int(input(\"\\nEnter the number of the model you want to use: \")) - 1\n","selected_model_key = list(model_choices.keys())[model_idx]\n","selected_model = model_choices[selected_model_key]\n","\n","# 2. Select Video\n","print(\"\\nAvailable Videos:\")\n","for idx, video_name in enumerate(video_choices.keys()):\n","    print(f\"{idx+1}. {video_name}\")\n","\n","video_idx = int(input(\"\\nEnter the number of the video you want to analyze: \")) - 1\n","selected_video_key = list(video_choices.keys())[video_idx]\n","selected_video = video_choices[selected_video_key]\n","\n","# 3. Run Pipeline\n","print(f\"\\nRunning VIW with {selected_model_key} on {selected_video_key}...\")\n","full_captions, unique_captions, final_story = run_viw_pipeline(\n","    video_path=selected_video,\n","    model_choice=selected_model,\n","    fps=1\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FB9cIKRThlC","executionInfo":{"status":"ok","timestamp":1745810584398,"user_tz":240,"elapsed":28422,"user":{"displayName":"Irving Villanueva","userId":"08778789764380129001"}},"outputId":"d7c97e4a-4d9d-4e26-ed15-db9a0689261c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Available Models:\n","1. Vanilla BLIP\n","2. Fine-Tuned BLIP\n","\n","Enter the number of the model you want to use: 2\n","\n","Available Videos:\n","1. A man and his dog walk through the snow in front of a cabin\n","2. Close-up View of Chef Preparing Food\n","3. Child Playing on a Tranquil Beach\n","4. Busy Nightlife Street in Rain with Umbrellas\n","5. Tourists at Teotihuacan's Pyramid of the Sun\n","\n","Enter the number of the video you want to analyze: 1\n","\n","Running VIW with Fine-Tuned BLIP on A man and his dog walk through the snow in front of a cabin...\n","VIW Activated! \n","Analyzing...\n","Extracting Frames...\n","Extracted 22 frames to /content/frames_output/\n","\n","Generating Captions for Each Frame:\n","Frame 1: a long shot shows a family in front of a red cabin in the snow with a dog in front of it, a brown dog, and a white cabin in the background, there is a dark blue door, and a white window on the left\n","Frame 2: a medium outdoor shot shows a group of people and a dog in front of a red cabin in the snow with a dog in front of them, surrounded by tall trees, there is a dark brown wooden cabin with a dark blue door and a dark\n","Frame 3: a medium outdoor shot shows a family in front of a red cabin in the snow with their dog, and a dog in front of the cabin, there is a large dark green door, and a dark green door, and a dark green door on\n","Frame 4: a long - haired woman and her two - legged dog walk toward the camera in front of a modern, dark - brown wooden cabin in the snow - colored forest - like setting, with a dark green door, and a dark green - brown dog\n","Frame 5: a long - outdoor shot shows a family in front of a cabin in the snow with a dog on the left, and a dog on the right side of the frame, in front of the cabin, there is a dark brown wooden fenced door\n","Frame 6: a long - outdoor shot shows a woman and her dog in front of a red - and - white cabin in the snow - covered forest, with a dog in front of a dark green door, a dark green - red - and - white background\n","Frame 7: a woman and her dog in front of a red cabin in the snow\n","Frame 8: a long shot shows two people walking in the snow towards a cabin with a dog in front of them, one of the cabin is a dark brown, and the second of the cabin is made of wood, and the cabin is made of dark wood\n","Frame 9: a long - haired woman and her two - legged dog walk toward a modern, dark - brown - brown - and - white cabin in the foreground, in a snowy - white forest, toward the right, toward the right, toward the camera\n","Frame 10: a medium shot shows a man and his dog in front of a red log cabin in the snow, with a brown dog in front of them, and a white background, there is a dark brown cabin with a dark brown roof and white trim around\n","Frame 11: a full shot shows a snowy - covered cabin with a dog in front of it, a brown and white tree trunks, and a brown dog in the foreground, with a white scarf around its neck, and a black collar around its neck,\n","Frame 12: a man and woman stand in the snow in front of a red - and - white cabin with a dog in front of them, a snowy - white - brown - and - brown cabin, a black - and - white backdrop of - red -\n","Frame 13: a long shot shows a woman and her dog in front of a red cabin in the snow, with a snowy backdrop of tall trees and a fence in the background, there is a red cabin with a black door, and a black window, and\n","Frame 14: a long shot shows two people standing in front of a red cabin in the snow with a dog in front of them, a dog and a man in the foreground, a dog in the foreground is a black and a white snow - covered\n","Frame 15: a long shot shows a woman and her two children in front of a red cabin in the snow with their dog in front of a snowy - covered cabin in a forest of them\n","Frame 16: a long - haired woman stands in front of a red - and - white cabin in a snowy - white forest - like environment, with a black - and - white backdrop, a black - and - white backdrop, a black - white backdrop,\n","Frame 17: a long - haired woman and her two - legged dog in front of a red - and - white cabin in a snowy - white forest, a dark - brown - brown forest, a dark - brown cabin, a dark - brown cabin, and\n","Frame 18: a long shot shows a woman and her two dogs in front of a red - and - white cabin in the snow, with a dark green - brown dog on the left side of the cabin, and a dark brown wooden porch on the right,\n","Frame 19: a long - haired woman and her two - legged children stand in front of a red - and - white cabin in a snowy - covered forest - white - covered forest - snow - covered forest, with a dark blue - brown dog, and -\n","Frame 20: a long shot shows a group of people standing in front of a red and white cabin in the snow in a forest of tall trees, with a dark brown structure in the foreground, there is a dark brown wooden structure with a dark blue door\n","Frame 21: a long - outdoor shot shows a woman and her dog in front of a red - and - white cabin in the snow - covered forest, with a backdrop of tall, tall, dark brown trees, and a dark brown wooden structure. the cabin\n","Frame 22: a long shot shows a woman walking toward a small, dark - red, wooden cabin in the snow, with her dog in front of her, she is a dark brown, and white cabin, and she has a red stripe pattern on the side\n","\n","Filtering Unique Captions Based on Similarity...\n"," Skipped Similar Caption: a medium outdoor shot shows a group of people and a dog in front of a red cabin in the snow with a dog in front of them, surrounded by tall trees, there is a dark brown wooden cabin with a dark blue door and a dark\n"," Skipped Similar Caption: a medium outdoor shot shows a family in front of a red cabin in the snow with their dog, and a dog in front of the cabin, there is a large dark green door, and a dark green door, and a dark green door on\n"," Skipped Similar Caption: a long - haired woman and her two - legged dog walk toward the camera in front of a modern, dark - brown wooden cabin in the snow - colored forest - like setting, with a dark green door, and a dark green - brown dog\n"," Skipped Similar Caption: a long - outdoor shot shows a family in front of a cabin in the snow with a dog on the left, and a dog on the right side of the frame, in front of the cabin, there is a dark brown wooden fenced door\n"," Skipped Similar Caption: a long - outdoor shot shows a woman and her dog in front of a red - and - white cabin in the snow - covered forest, with a dog in front of a dark green door, a dark green - red - and - white background\n"," Skipped Similar Caption: a woman and her dog in front of a red cabin in the snow\n"," Skipped Similar Caption: a long shot shows two people walking in the snow towards a cabin with a dog in front of them, one of the cabin is a dark brown, and the second of the cabin is made of wood, and the cabin is made of dark wood\n"," Skipped Similar Caption: a long - haired woman and her two - legged dog walk toward a modern, dark - brown - brown - and - white cabin in the foreground, in a snowy - white forest, toward the right, toward the right, toward the camera\n"," Skipped Similar Caption: a medium shot shows a man and his dog in front of a red log cabin in the snow, with a brown dog in front of them, and a white background, there is a dark brown cabin with a dark brown roof and white trim around\n"," Skipped Similar Caption: a full shot shows a snowy - covered cabin with a dog in front of it, a brown and white tree trunks, and a brown dog in the foreground, with a white scarf around its neck, and a black collar around its neck,\n"," Skipped Similar Caption: a man and woman stand in the snow in front of a red - and - white cabin with a dog in front of them, a snowy - white - brown - and - brown cabin, a black - and - white backdrop of - red -\n"," Skipped Similar Caption: a long shot shows a woman and her dog in front of a red cabin in the snow, with a snowy backdrop of tall trees and a fence in the background, there is a red cabin with a black door, and a black window, and\n"," Skipped Similar Caption: a long shot shows two people standing in front of a red cabin in the snow with a dog in front of them, a dog and a man in the foreground, a dog in the foreground is a black and a white snow - covered\n"," Skipped Similar Caption: a long shot shows a woman and her two children in front of a red cabin in the snow with their dog in front of a snowy - covered cabin in a forest of them\n"," Skipped Similar Caption: a long - haired woman stands in front of a red - and - white cabin in a snowy - white forest - like environment, with a black - and - white backdrop, a black - and - white backdrop, a black - white backdrop,\n"," Skipped Similar Caption: a long - haired woman and her two - legged dog in front of a red - and - white cabin in a snowy - white forest, a dark - brown - brown forest, a dark - brown cabin, a dark - brown cabin, and\n"," Skipped Similar Caption: a long shot shows a woman and her two dogs in front of a red - and - white cabin in the snow, with a dark green - brown dog on the left side of the cabin, and a dark brown wooden porch on the right,\n"," Skipped Similar Caption: a long - haired woman and her two - legged children stand in front of a red - and - white cabin in a snowy - covered forest - white - covered forest - snow - covered forest, with a dark blue - brown dog, and -\n"," Skipped Similar Caption: a long shot shows a group of people standing in front of a red and white cabin in the snow in a forest of tall trees, with a dark brown structure in the foreground, there is a dark brown wooden structure with a dark blue door\n"," Skipped Similar Caption: a long - outdoor shot shows a woman and her dog in front of a red - and - white cabin in the snow - covered forest, with a backdrop of tall, tall, dark brown trees, and a dark brown wooden structure. the cabin\n"," Skipped Similar Caption: a long shot shows a woman walking toward a small, dark - red, wooden cabin in the snow, with her dog in front of her, she is a dark brown, and white cabin, and she has a red stripe pattern on the side\n","\n","Unique Captions After Filtering:\n","Unique 1: a long shot shows a family in front of a red cabin in the snow with a dog in front of it, a brown dog, and a white cabin in the background, there is a dark blue door, and a white window on the left\n","\n","Merging and Cleaning Captions into a Final Story...\n","Unique Captions Used: 1\n","['A long shot shows a family in front of a red cabin in the snow with a dog in front of it, a brown dog, and a white cabin in the background, there is a dark blue door, and a white window on the left.']\n","\n","Final Smoothed Video Caption:\n","A long shot shows a family in front of a red cabin in the snow with a dog in front of it, a brown dog, and a white cabin in the background, there is a dark blue door, and a white window on the left.\n"]}]}]}